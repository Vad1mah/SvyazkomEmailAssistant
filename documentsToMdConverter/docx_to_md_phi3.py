#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–ö–æ–Ω–≤–µ—Ä—Ç–µ—Ä .docx –≤ .md —Å –∞–Ω–∞–ª–∏–∑–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ Phi-3.
–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π, –ª–æ–≥–∞–º–∏ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
"""

import os
import sys
import logging
import re
from pathlib import Path
from typing import List, Tuple
import io
from datetime import datetime
import argparse
from tqdm import tqdm
from multiprocessing import Pool, cpu_count

from PIL import Image
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from docx import Document
from docx.oxml.text.paragraph import CT_P
from docx.oxml.table import CT_Tbl
from docx.table import Table
from docx.text.paragraph import Paragraph
import zipfile
import hashlib

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('conversion_phi3.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("docx_to_md_phi3")

class DocxConverterPhi3:
    UNWANTED_SECTION_HEADERS = [
        r'^Security Classification',
        r'^Restricted Information',
        r'^Copyright\s+Notice',
        r'^Document History',
        r'^Other Information',
        r'^Keywords',
        r'^Revision control',
        r'^Table of Contents'
    ]
    
    def __init__(self, input_dir="docx", output_dir="md_phi3", dry_run=False, workers=1):
        self._init_phi3()
        self.md_dir = Path(output_dir)
        if not dry_run:
            self.md_dir.mkdir(exist_ok=True)
        self.input_dir = Path(input_dir)
        self.dry_run = dry_run
        self.workers = workers
        self.stats = {
            'processed_files': 0,
            'extracted_images': 0,
            'ai_analyses': 0,
            'errors': 0
        }

    def _init_phi3(self):
        try:
            logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Phi-3...")
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
            model_id = "microsoft/phi-3-mini-4k-instruct"
            self.tokenizer = AutoTokenizer.from_pretrained(model_id)
            self.model = AutoModelForCausalLM.from_pretrained(
                model_id,
                device_map="auto",
                torch_dtype=torch.float16 if self.device=="cuda" else torch.float32
            )
            logger.info("Phi-3 –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
        except Exception as e:
            logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å Phi-3: {e}", exc_info=True)
            self.model = None
            self.tokenizer = None

    def _is_unwanted_paragraph(self, text: str) -> bool:
        txt = text.strip()
        return any(re.match(pat, txt, re.IGNORECASE)
                   for pat in self.UNWANTED_SECTION_HEADERS)

    def _extract_all_images_from_docx(self, docx_path: Path) -> List[dict]:
        images = []
        hash_counts = {}
        try:
            with zipfile.ZipFile(docx_path, 'r') as zip_file:
                for file_info in zip_file.filelist:
                    if file_info.filename.startswith('word/media/'):
                        data = zip_file.read(file_info.filename)
                        h = hashlib.md5(data).hexdigest()
                        hash_counts[h] = hash_counts.get(h, 0) + 1
                        images.append({'data': data, 'size': len(data), 'hash': h})
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è media –≤ {docx_path.name}: {e}")
        for img in images:
            img['count'] = hash_counts[img['hash']]
        return images

    def extract_text_and_images_from_docx(
        self,
        docx_path: Path
    ) -> Tuple[str, List[Tuple[int, bytes, int, str, str]], dict]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
         - —Å–∫–ª–µ–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –±–µ–∑ ¬´–º—É—Å–æ—Ä–Ω—ã—Ö¬ª –±–ª–æ–∫–æ–≤,
         - —Å–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (–Ω–æ–º–µ—Ä, data, –ø–æ–∑–∏—Ü–∏—è, –∫–æ–Ω—Ç–µ–∫—Å—Ç, —Ö—ç—à),
         - —Å–ª–æ–≤–∞—Ä—å hash->(–Ω–æ–º–µ—Ä, data, –ø–æ–∑–∏—Ü–∏—è, –∫–æ–Ω—Ç–µ–∫—Å—Ç)
        """
        skip_block = False
        try:
            doc = Document(str(docx_path))
            image_files = self._extract_all_images_from_docx(docx_path)
            text_parts: List[str] = []
            image_hash_to_info = {}
            last_text_block = ""
            img_idx = 0
            image_counter = 0
            logo_hash = image_files[0]['hash'] if image_files else None

            for element in doc.element.body:

                # --- –ü–ê–†–ê–ì–†–ê–§–´ ---
                if isinstance(element, CT_P):
                    paragraph = Paragraph(element, doc)
                    text = paragraph.text.strip()

                    # —Å—Ç–∞—Ä—Ç/—Å—Ç–æ–ø –ø—Ä–æ–ø—É—Å–∫–∞ —Ç–µ–∫—Å—Ç–∞
                    if self._is_unwanted_paragraph(text):
                        logger.info(f"Skip text section: {text}")
                        skip_block = True
                    elif skip_block and (
                        re.match(r'^\d+(\.\d+)*\s+', text) or
                        re.match(r'^[A-Z][A-Z\s]{8,}$', text)
                    ):
                        skip_block = False

                    # –¥–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç, —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ –≤ skip_block
                    if not skip_block and text:
                        text_parts.append(text)
                        last_text_block = text

                    # –Ω–æ –≤ –ª—é–±–æ–º —Å–ª—É—á–∞–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞—Ä—Ç–∏–Ω–∫–∏ –≤ —ç—Ç–æ–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ–µ
                    for run in paragraph.runs:
                        for child in run._element:
                            if child.tag.endswith('drawing') or child.tag.endswith('pict'):
                                if img_idx < len(image_files):
                                    info = image_files[img_idx]
                                    h = info['hash']
                                    # –ª–æ–≥–æ—Ç–∏–ø –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–æ–ª–Ω–æ—Å—Ç—å—é
                                    if not (logo_hash and h == logo_hash):
                                        if h not in image_hash_to_info:
                                            image_counter += 1
                                            image_hash_to_info[h] = (
                                                image_counter,
                                                info['data'],
                                                len(text_parts),
                                                last_text_block
                                            )
                                            text_parts.append(f"[IMAGE_{image_counter}]")
                                        else:
                                            num = image_hash_to_info[h][0]
                                            text_parts.append(f"[IMAGE_{num}]")
                                    img_idx += 1
                                break
                        # –µ—Å–ª–∏ –Ω–∞—à–ª–∏ –∫–∞—Ä—Ç–∏–Ω–∫—É ‚Äî –ø–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —Å–ª–µ–¥—É—é—â–µ–º—É run
                    continue

                # --- –¢–ê–ë–õ–ò–¶–´ ---
                if isinstance(element, CT_Tbl):
                    table = Table(element, doc)
                    first_cell = table.rows[0].cells[0].text.strip() if table.rows else ""
                    # –µ—Å–ª–∏ —ç—Ç–æ ¬´Document History¬ª –∏–ª–∏ –ø–æ–¥–æ–±–Ω–æ–µ ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –≤—Å—é —Ç–∞–±–ª–∏—Ü—É
                    if any(re.match(pat, first_cell, re.IGNORECASE)
                           for pat in self.UNWANTED_SECTION_HEADERS):
                        logger.info(f"Skip unwanted table: {first_cell}")
                        continue

                    # –æ–±—ã—á–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞
                    table_text = self._extract_table_text(table)
                    if table_text:
                        text_parts.append(table_text)
                        last_text_block = table_text

                    # –≤ —Ç–∞–±–ª–∏—Ü–∞—Ö —Ç–æ–∂–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—Ä—Ç–∏–Ω–∫–∏
                    for row in table.rows:
                        for cell in row.cells:
                            for para in cell.paragraphs:
                                for run in para.runs:
                                    for child in run._element:
                                        if child.tag.endswith('drawing') or child.tag.endswith('pict'):
                                            if img_idx < len(image_files):
                                                info = image_files[img_idx]
                                                h = info['hash']
                                                if not (logo_hash and h == logo_hash):
                                                    if h not in image_hash_to_info:
                                                        image_counter += 1
                                                        image_hash_to_info[h] = (
                                                            image_counter,
                                                            info['data'],
                                                            len(text_parts),
                                                            last_text_block
                                                        )
                                                        text_parts.append(f"[IMAGE_{image_counter}]")
                                                    else:
                                                        num = image_hash_to_info[h][0]
                                                        text_parts.append(f"[IMAGE_{num}]")
                                                img_idx += 1
                                            break

            # —Ñ–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ –∏–∑ map
            images = [
                (num, data, pos, ctx, h)
                for h, (num, data, pos, ctx) in image_hash_to_info.items()
            ]
            return '\n'.join(text_parts), images, image_hash_to_info

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–∑ {docx_path.name}: {e}", exc_info=True)
            return "", [], {}
 
    def _extract_table_text(self, table: Table) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç markdown-—Ç–∞–±–ª–∏—Ü—É, –ø—Ä–æ–ø—É—Å–∫–∞—è ¬´–∏—Å—Ç–æ—Ä–∏—é –≤–µ—Ä—Å–∏–π¬ª –∏ —Ç.–ø."""
        if not table.rows:
            return ""
        # –ï—Å–ª–∏ –ø–µ—Ä–≤–∞—è —è—á–µ–π–∫–∞ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–µ–Ω—É–∂–Ω—ã–π –±–ª–æ–∫ ‚Äî —Å—Ä–∞–∑—É –≤—ã—Ö–æ–¥–∏–º
        first_cell = table.rows[0].cells[0].text.strip()
        if any(re.match(pattern, first_cell, re.IGNORECASE)
               for pattern in self.UNWANTED_SECTION_HEADERS):
            return ""

        # –í –ø—Ä–æ—Ç–∏–≤–Ω–æ–º —Å–ª—É—á–∞–µ —Å—Ç—Ä–æ–∏–º markdown-—Ç–∞–±–ª–∏—Ü—É
        table_lines = []
        for i, row in enumerate(table.rows):
            cells = [cell.text.strip() for cell in row.cells]
            if cells:
                line = '| ' + ' | '.join(cells) + ' |'
                table_lines.append(line)
                if i == 0:
                    sep = '| ' + ' | '.join(['---'] * len(cells)) + ' |'
                    table_lines.append(sep)
        return '\n'.join(table_lines)

    def postprocess_image_description(self, desc: str) -> str:
        # –£–¥–∞–ª—è–µ–º –Ω–µ–∞–Ω–≥–ª–∏–π—Å–∫–∏–π —Ç–µ–∫—Å—Ç (–µ—Å–ª–∏ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è)
        # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫–∏, –≥–¥–µ >60% —Å–∏–º–≤–æ–ª–æ–≤ ‚Äî –ª–∞—Ç–∏–Ω–∏—Ü–∞/—Ü–∏—Ñ—Ä—ã/–∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
        lines = desc.split('\n')
        eng_lines = []
        for l in lines:
            l_strip = l.strip()
            if not l_strip:
                continue
            latin = sum(c.isalpha() and c in 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ' for c in l_strip)
            total = sum(c.isalpha() for c in l_strip)
            if total == 0 or (latin / total > 0.6):
                eng_lines.append(l_strip)
        desc = '\n'.join(eng_lines)
        # –£–¥–∞–ª—è–µ–º –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Å—Ç—Ä–æ–∫–∏
        seen = set()
        uniq_lines = []
        for l in desc.split('\n'):
            if l not in seen:
                uniq_lines.append(l)
                seen.add(l)
        desc = '\n'.join(uniq_lines)
        # –£–¥–∞–ª—è–µ–º —à–∞–±–ª–æ–Ω–Ω—ã–µ —Ñ—Ä–∞–∑—ã (case-insensitive)
        desc = re.sub(r'(?i)^(this is|the image shows|figure|diagram|picture|photo|image|document)[\s:Ôºö-]*', '', desc)
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
        desc = re.sub(r'\n{2,}', '\n', desc)
        desc = desc.strip()
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –æ–ø–∏—Å–∞–Ω–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, 600 —Å–∏–º–≤–æ–ª–æ–≤)
        if len(desc) > 600:
            desc = desc[:600].rsplit(' ', 1)[0] + '...'
        return desc

    def _is_meaningless_image(self, image_data: bytes) -> bool:
        # –ü—Ä–æ–≤–µ—Ä–∫–∞: —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–ª–∏ –ø–æ—á—Ç–∏ –ø–æ–ª–Ω–æ—Å—Ç—å—é –±–µ–ª–æ–µ/—á—ë—Ä–Ω–æ–µ
        try:
            img = Image.open(io.BytesIO(image_data))
            if img.width < 100 or img.height < 100:
                return True
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–æ—á—Ç–∏ –æ–¥–Ω–æ—Ç–æ–Ω–Ω–æ—Å—Ç—å
            grayscale = img.convert('L')
            hist = grayscale.histogram()
            total = sum(hist)
            # –ï—Å–ª–∏ 95%+ –ø–∏–∫—Å–µ–ª–µ–π –æ–¥–Ω–æ–≥–æ —Ü–≤–µ—Ç–∞ (–ø–æ—á—Ç–∏ –±–µ–ª–æ–µ/—á—ë—Ä–Ω–æ–µ)
            if max(hist) / total > 0.95:
                return True
        except Exception:
            return False
        return False

    def analyze_image_with_phi3(self, image_data: bytes, context_text: str = "") -> str:
        try:
            temp_path = "temp_image.jpg"
            with open(temp_path, 'wb') as f:
                f.write(image_data)
            prompt = (
                "Provide a concise, structured, and informative description of the diagram or image for a technical knowledge base. "
                "If the image is a diagram, flowchart, schema, table, or technical illustration, describe its structure in detail. "
                "If the image is a regular photo, background, logo, or does not contain technical content, reply: 'This image is not a technical diagram, chart, table, or illustration and will be skipped.' "
                "- List all main components and their relationships (hierarchy, connections, branches). "
                "- Enumerate all key fields, nodes, or blocks, specifying their names and any labels such as 'Mandatory (M)', 'Optional (O)', 'Conditional (C)', or 'Recommended (R)'. "
                "- Clearly explain the meaning of each field or label if possible. "
                "- Do not include introductions, repeated questions, or template phrases. "
                "Write in clear, professional English, focusing on technical accuracy and completeness. "
                f"Context: {context_text[:200]}\n<file://{temp_path}>\nDescription:"
            )
            if self.tokenizer is None or self.model is None:
                logger.error("Phi-3 model or tokenizer is not initialized.")
                os.remove(temp_path)
                return "[Image description unavailable]"
            try:
                inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
                output = self.model.generate(**inputs, max_new_tokens=120)
                response = self.tokenizer.decode(output[0], skip_special_tokens=True)
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ–ø–∏—Å–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
                os.remove(temp_path)
                return "[Image description unavailable]"
            os.remove(temp_path)
            self.stats['ai_analyses'] += 1
            # –û–±—Ä–µ–∑–∞–µ–º prompt –∏–∑ –æ—Ç–≤–µ—Ç–∞
            if 'Description:' in response:
                response = response.split('Description:')[-1].strip()
            # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å —è–≤–Ω–æ —Å–∫–∞–∑–∞–ª–∞, —á—Ç–æ —ç—Ç–æ –Ω–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
            if 'not a technical diagram' in response.lower() or 'will be skipped' in response.lower():
                return '[Image skipped: not a technical diagram]'
            # –£–¥–∞–ª—è–µ–º —à–∞–±–ª–æ–Ω–Ω—ã–µ —Å–ª–æ–≤–∞ –∏ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
            response = re.sub(r'(?i)^(document|title|introduction|description|image|figure)[\s:Ôºö-]*', '', response)
            response = re.sub(r'(?i)^(document|title|introduction|description|image|figure)[\s:Ôºö-]*', '', response)
            response = re.sub(r'^[\s>\-*]+', '', response, flags=re.MULTILINE)
            response = re.sub(r'\n{2,}', '\n', response)
            # –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ–ø–∏—Å–∞–Ω–∏—è
            response = self.postprocess_image_description(response)
            return response.strip()
        except Exception as e:
            logger.error(f"Phi-3 –∞–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–µ —É–¥–∞–ª—Å—è: {e}")
            return "[Image description unavailable]"

    def clean_and_structure_text(self, text: str) -> str:
        """
        –û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –æ—Ç –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è –Ω–µ–Ω—É–∂–Ω—ã—Ö —Å–µ–∫—Ü–∏–π –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–µ—Ç –≤ Markdown:
        –∑–∞–≥–æ–ª–æ–≤–∫–∏, —Å–ø–∏—Å–∫–∏, —Ç–∞–±–ª–∏—Ü—ã –∏ —Ç.–¥.
        """
        # 1) –í—ã—Ä–µ–∑–∞–µ–º –±–ª–æ–∫–∏ –ø–æ UNWANTED_SECTION_HEADERS
        lines = text.split('\n')
        pruned: List[str] = []
        skip = False
        for line in lines:
            stripped = line.strip()
            # –ù–∞—á–∞–ª–æ –Ω–µ–Ω—É–∂–Ω–æ–π —Å–µ–∫—Ü–∏–∏
            if any(re.match(pat, stripped, re.IGNORECASE)
                   for pat in self.UNWANTED_SECTION_HEADERS):
                skip = True
                continue
            # –ö–æ–Ω–µ—Ü: –∑–∞–≥–æ–ª–æ–≤–æ–∫ –≤ Markdown (#, ## –∏ —Ç.–ø.)
            if skip and re.match(r'^#{1,6}\s+\w+', stripped):
                skip = False
            if not skip:
                pruned.append(line)

        # 2) –û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –≤ Markdown
        cleaned_lines: List[str] = []
        in_table = False
        for idx, raw in enumerate(pruned):
            line = raw.rstrip()

            # –¢–∞–±–ª–∏—Ü–∞ Markdown
            if line.strip().startswith('|') and line.strip().endswith('|'):
                cleaned_lines.append(line)
                in_table = True
                continue
            if in_table and not (line.strip().startswith('|') and line.strip().endswith('|')):
                in_table = False

            # –ù—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –≤–∏–¥–∞ 1. 1.1. A. I.
            m = re.match(r'^(\d+(?:\.\d+)*|[A-Z]|[IVX]+)\.?\s+(.+)', line)
            if m:
                prefix = m.group(1)
                header = m.group(2).strip()
                level = prefix.count('.') + 1 if '.' in prefix else 2
                level = min(level, 6)
                cleaned_lines.append(f"{'#' * level} {header}")
                continue

            # –Ø–≤–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏: –≤—Å–µ CAPS >8 —Å–∏–º–≤–æ–ª–æ–≤
            if re.match(r'^[A-Z][A-Z\s]{8,}$', line):
                cleaned_lines.append(f"# {line.title()}")
                continue

            # –ú–∞—Ä–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫
            if re.match(r'^\s*[-*‚Ä¢]\s+', line):
                cleaned_lines.append(re.sub(r'^\s*[-*‚Ä¢]\s+', '- ', line))
                continue

            # –ù—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫
            if re.match(r'^\s*\d+\.', line):
                cleaned_lines.append(re.sub(r'^\s*(\d+)\.', r'\1.', line))
                continue

            # –í–ª–æ–∂–µ–Ω–Ω—ã–µ —Å–ø–∏—Å–∫–∏
            m_indent = re.match(r'^(\s+)[-*‚Ä¢]\s+', line)
            if m_indent:
                indent = len(m_indent.group(1)) // 2
                cleaned_lines.append('  ' * indent + '- ' + line.lstrip(' -*‚Ä¢'))
                continue

            # –û–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç
            cleaned_lines.append(line)

        # 3) –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
        result: List[str] = []
        prev_blank = False
        for l in cleaned_lines:
            if l.strip() == '':
                if not prev_blank:
                    result.append('')
                prev_blank = True
            else:
                result.append(l)
                prev_blank = False

        return '\n'.join(result).strip()
 
    def _process_document_worker(self, docx_file):
        # –î–ª—è multiprocessing: –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –≤ –∫–∞–∂–¥–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è, —Ç–æ–ª—å–∫–æ –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞
        try:
            text, images, image_hash_to_info = self.extract_text_and_images_from_docx(docx_file)
            if not text:
                return (docx_file.name, None, 0, 0, 1)  # errors=1
            image_descriptions = {}
            extracted_images = 0
            ai_analyses = 0
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            for num, img_data, position, context, h in images:
                if self._is_meaningless_image(img_data):
                    image_descriptions[num] = "[Image skipped: meaningless photo]"
                    continue
                try:
                    description = self.analyze_image_with_phi3(img_data, context)
                    image_descriptions[num] = description
                    extracted_images += 1
                    ai_analyses += 1
                except Exception:
                    image_descriptions[num] = "[Image description unavailable]"
            # –ü–æ–¥—Å—Ç–∞–≤–ª—è–µ–º –æ–ø–∏—Å–∞–Ω–∏—è –ø–æ –Ω–æ–º–µ—Ä–∞–º –≤ —Ç–µ–∫—Å—Ç
            def replace_marker(match):
                try:
                    image_num = int(match.group(1))
                    desc = image_descriptions.get(image_num, "")
                    if desc.startswith('[Image skipped:'):
                        return ""
                    if image_num in image_descriptions:
                        return str(desc).strip()
                    else:
                        return ""
                except ValueError:
                    return match.group(0)
            import re
            final_text = re.sub(r'\[IMAGE_(\d+)\]', replace_marker, text)
            # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏, –≥–¥–µ –æ—Å—Ç–∞–ª–∏—Å—å '[Image skipped:' –∏–ª–∏ –ø—É—Å—Ç—ã–µ –º–∞—Ä–∫–µ—Ä—ã
            final_text = '\n'.join([line for line in final_text.splitlines() if '[Image skipped:' not in line and not re.match(r'^\s*\[IMAGE_\d+\]\s*$', line)])
            cleaned_text = self.clean_and_structure_text(final_text)
            md_content = self._create_markdown_content(docx_file, cleaned_text, extracted_images)
            return (docx_file.name, md_content, extracted_images, ai_analyses, 0)
        except Exception:
            return (docx_file.name, None, 0, 0, 1)

    def _create_markdown_content(self, docx_path: Path, text: str, image_count: int) -> str:
        title = docx_path.stem.replace('_', ' ').title()
        metadata = f"# {title}\n\n"
        return metadata + text

    def convert_all_documents(self, target_file: str = "", file_filter: str = "*.docx"):
        docx_dir = self.input_dir
        if not docx_dir.exists():
            logger.error(f"–ü–∞–ø–∫–∞ {docx_dir}/ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            return
        if target_file:
            docx_files = [docx_dir / target_file]
            if not docx_files[0].exists():
                logger.error(f"–§–∞–π–ª {target_file} –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ø–∞–ø–∫–µ {docx_dir}")
                return
        else:
            docx_files = list(docx_dir.glob(file_filter))
        docx_files = [f for f in docx_files if not f.name.startswith('~$')]
        if not docx_files:
            logger.warning("–î–æ–∫—É–º–µ–Ω—Ç—ã .docx –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return
        logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(docx_files)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏")
        results = []
        if self.workers > 1:
            with Pool(processes=self.workers) as pool:
                for res in tqdm(pool.imap(self._process_document_worker, sorted(docx_files)), total=len(docx_files), desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–æ–≤", unit="—Ñ–∞–π–ª"):
                    results.append(res)
        else:
            for docx_file in tqdm(sorted(docx_files), desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–æ–≤", unit="—Ñ–∞–π–ª"):
                res = self._process_document_worker(docx_file)
                results.append(res)
        for docx_file_name, md_content, extracted_images, ai_analyses, errors in results:
            if md_content:
                if not self.dry_run:
                    md_file = self.md_dir / f"{Path(docx_file_name).stem}.md"
                    with open(md_file, 'w', encoding='utf-8') as f:
                        f.write(md_content)
                    logger.info(f"–°–æ–∑–¥–∞–Ω —Ñ–∞–π–ª: {md_file}")
                self.stats['processed_files'] += 1
                self.stats['extracted_images'] += extracted_images
                self.stats['ai_analyses'] += ai_analyses
            else:
                self.stats['errors'] += errors
        self._print_statistics()

    def _print_statistics(self):
        logger.info("=" * 50)
        logger.info("–°–¢–ê–¢–ò–°–¢–ò–ö–ê –û–ë–†–ê–ë–û–¢–ö–ò")
        logger.info("=" * 50)
        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {self.stats['processed_files']}")
        logger.info(f"–ò–∑–≤–ª–µ—á–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {self.stats['extracted_images']}")
        logger.info(f"–ò–ò-–∞–Ω–∞–ª–∏–∑–æ–≤ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ: {self.stats['ai_analyses']}")
        logger.info(f"–û—à–∏–±–æ–∫: {self.stats['errors']}")
        logger.info("=" * 50)

def main():
    parser = argparse.ArgumentParser(description="–ö–æ–Ω–≤–µ—Ä—Ç–µ—Ä .docx –≤ .md —Å –∞–Ω–∞–ª–∏–∑–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ Phi-3")
    parser.add_argument('filename', type=str, nargs='?', default=None,
                        help='–ò–º—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ .docx —Ñ–∞–π–ª–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏. –ï—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–æ, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è –≤—Å–µ —Ñ–∞–π–ª—ã –≤ –ø–∞–ø–∫–µ docx/.')
    parser.add_argument('--input', type=str, default='docx', help='–ü–∞–ø–∫–∞ —Å –∏—Å—Ö–æ–¥–Ω—ã–º–∏ .docx —Ñ–∞–π–ª–∞–º–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: docx)')
    parser.add_argument('--output', type=str, default='md_phi3', help='–ü–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è .md —Ñ–∞–π–ª–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: md_phi3)')
    parser.add_argument('--filter', type=str, default='*.docx', help='–ú–∞—Å–∫–∞ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Ñ–∞–π–ª–æ–≤, –Ω–∞–ø—Ä–∏–º–µ—Ä: "*report*.docx"')
    parser.add_argument('--dry-run', action='store_true', help='–ù–µ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å .md —Ñ–∞–π–ª—ã, —Ç–æ–ª—å–∫–æ –≤—ã–≤–æ–¥–∏—Ç—å –ø—Ä–æ—Ü–µ—Å—Å')
    parser.add_argument('--workers', type=int, default=1, help='–ß–∏—Å–ª–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 1, –º–∞–∫—Å–∏–º—É–º: —á–∏—Å–ª–æ —è–¥–µ—Ä)')
    args = parser.parse_args()
    print("üöÄ –ö–æ–Ω–≤–µ—Ä—Ç–µ—Ä .docx –≤ .md —Å –∞–Ω–∞–ª–∏–∑–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ Phi-3")
    print("=" * 60)
    try:
        converter = DocxConverterPhi3(input_dir=args.input, output_dir=args.output, dry_run=args.dry_run, workers=args.workers)
        converter.convert_all_documents(target_file=args.filename, file_filter=args.filter)
        print("‚úÖ –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏: {e}")
        logger.critical(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}", exc_info=True)

if __name__ == "__main__":
    main() 